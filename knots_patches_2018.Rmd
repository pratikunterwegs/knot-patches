--- 
knit: "bookdown::render_book"
title: "Residence patches for red knots"
author: "Pratik R Gupte"
date: "`r Sys.Date()`"
site: bookdown::bookdown_site
output: bookdown::gitbook
documentclass: book
bibliography: [refs.bib]
biblio-style: apalike
link-citations: yes
github-repo: pratikunterwegs/knot-association
description: "Residence patches for red knots in the Wadden Sea."
---
```{r include=FALSE, cache=FALSE}
knitr::opts_knit$set(root.dir = here::here())
set.seed(1)
options(digits = 3)

knitr::opts_chunk$set(
  comment = "#>",
  collapse = TRUE,
  eval = FALSE,
  cache = FALSE,
  warning = FALSE,
  error = FALSE,
  message = FALSE,
  out.width = "\\textwidth",
  fig.align = "center",
  fig.width = 7,
  fig.asp = 0.618, # 1 / phi
  fig.show = "hold"
)
options(knitr.kable.NA = "")
options(dplyr.print_min = 6, dplyr.print_max = 6)
```

# Introduction

This repository is the source for a project to idenify residence patches from high-resolution tracking data from individual red knots _Calidris canutus islandica_ tracked using the WATLAS system.

## Attribution

Please contact the following before cloning or in case of interest in the project.

Pratik Gupte (author and maintainer)  
[PhD student, GELIFES -- University of Groningen](https://www.rug.nl/staff/p.r.gupte)  
Guest researcher, COS -- NIOZ  
p.r.gupte@rug.nl  
Nijenborgh 7/5172.0583 9747AG Groningen

## Data access

The data used in this work are not publicly available. Contact PI [Allert Bijleveld (COS-NIOZ)](allert.bijleveld@nioz.nl) for data access.

## Data processing

The data processing for this project is described in the following sections. Navigate through them using the links in the sidebar.

<!--chapter:end:index.Rmd-->

---
editor_options: 
  chunk_output_type: console
---
```{r include=FALSE, cache=FALSE}
knitr::opts_knit$set(root.dir = here::here())
set.seed(1)
options(digits = 3)

knitr::opts_chunk$set(
  comment = "#>",
  collapse = TRUE,
  eval = FALSE,
  cache = FALSE,
  warning = FALSE,
  error = FALSE,
  message = FALSE,
  out.width = "\\textwidth",
  fig.align = "center",
  fig.width = 7,
  fig.asp = 0.618, # 1 / phi
  fig.show = "hold"
)
options(knitr.kable.NA = "")
options(dplyr.print_min = 6, dplyr.print_max = 6)
```

**OH BLOODY HELL THERE'S TIDYVERSE EVERYWHERE REMOVE IT ALL**

# Getting data

This section focusses on accessing and downloading WATLAS data. This is done using functions in the [WATLAS Utilities package](https://github.com/pratikunterwegs/watlastools). 

**Workflow**

1. Preparing required libraries.
2. Reading tag data with deployment start dates from a local file. This file is not yet publicly available.
3. Connecting to the NIOZ databse and downloading data. This database is also not public-access.

## Prepare `watlastools` and other libraries

```{r install_watlastools, message=FALSE, warning=FALSE}
# install the package watlastools from master branch using the following
# install.packages("devtools")
library(devtools)

devtools::install_github("pratikunterwegs/watlastools")
library(watlastools)

# libraries to process data
library(data.table)
library(ggplot2)
library(ggthemes)
library(purrr)
library(glue)
```

## Read in tag deployment data

```{r get_deployment_data, message=FALSE, warning=FALSE}
# read deployment data from local file in data folder
tag_info <- fread("data/data2018/SelinDB.csv")

# filter out NAs in release date and time
tag_info <- tag_info[!is.na(Release_Date) & !is.na(Release_Time), ]

# make release date column as POSIXct
tag_info[, Release_Date := as.POSIXct(paste(Release_Date,
  Release_Time,
  sep = " "
),
format = "%d.%m.%y %H:%M", tz = "CET"
)]

# check new release date column
head(tag_info$Release_Date)
```

```{r plot_release_schedule, echo=FALSE, fig.cap="Knots released per week of 2018.", message=FALSE, warning=FALSE}
# check release cohort
ggplot(tag_info) +
  geom_bar(aes(x = week(Release_Date)), col = 1, fill = "grey") +
  theme_test() +
  labs(
    x = "release week (2018)", y = "# knots released",
    caption = Sys.time()
  )
```

## Get data and save locally

```{r get_acess, warning=FALSE, message=FALSE}
# read in database access parameters from a local file
data_access <- fread("data/access_params.txt")
```

```{r get_data, message=FALSE, warning=FALSE}
# create a data storage file if not present
# use the getData function from watlastools on the tag_info data frame
# this is placed inside a pmap wrapper to automate access for all birds

if (!dir.exists("data/data2018")) {
  dir.create("data/data2018")
}

pmap(tag_info[, .(Toa_Tag, Release_Date)], function(Toa_Tag, Release_Date) {
  prelim_data <- watlastools::wat_get_data(
    tag = Toa_Tag,
    tracking_time_start = as.character(Release_Date),
    tracking_time_end = "2018-10-31",
    username = data_access$username,
    password = data_access$password
  )

  setDT(prelim_data)
  # prelim_data[,TAG:= = as.numeric(TAG) - prefix_num]

  message(glue("tag {Toa_Tag} accessed with {nrow(prelim_data)} fixes"))

  fwrite(prelim_data,
    file = glue("data/data2018/{Toa_Tag}_data.csv"),
    dateTimeAs = "ISO"
  )
})
```


<!--chapter:end:01_gettingData.Rmd-->

---
editor_options: 
  chunk_output_type: console
---
```{r include=FALSE, cache=FALSE}
knitr::opts_knit$set(root.dir = here::here())
set.seed(1)
options(digits = 3)

knitr::opts_chunk$set(
  comment = "#>",
  collapse = TRUE,
  eval = FALSE,
  cache = FALSE,
  warning = FALSE,
  error = FALSE,
  message = FALSE,
  out.width = "\\textwidth",
  fig.align = "center",
  fig.width = 7,
  fig.asp = 0.618, # 1 / phi
  fig.show = "hold"
)
options(knitr.kable.NA = "")
options(dplyr.print_min = 6, dplyr.print_max = 6)
```

# Cleaning data

This section is about cleaning downloaded data using the `cleanData` function in the [WATLAS Utilities package](https://github.com/pratikunterwegs/watlastools).

**Workflow**

1. Prepare required libraries.
2. Read in data, apply the cleaning function, and overwrite local data.

## Prepare `watlastools` and other libraries

```{r install_watlastools_2, message=FALSE, warning=FALSE}
# watlastools assumed installed from the previous step
# if not, install from the github repo as shown below

devtools::install_github("pratikunterwegs/watlastools")
library(watlastools)
```

```{r}
# libraries to process data
library(data.table)
library(glue)
library(fasttime)
library(stringr)
```

## Prepare to remove attractor points

```{r read_attractors, message=FALSE, warning=FALSE}
# read in identified attractor points
atp <- fread("data/attractor_points.txt")
```

## Read, clean, and write data

```{r read_in_raw_data, message=FALSE, warning=FALSE}
# make a list of data files to read
data_files <- list.files(
  path = "data/data_2018/data_tracks/",
  pattern = "whole_season*", full.names = TRUE
)

data_ids <- str_extract(data_files, "(tx_\\d+)") %>% str_sub(-3, -1)

# read deployment data from local file in data folder
tag_info <- fread("data/data_2018/SelinDB.csv")

# filter out NAs in release date and time
tag_info <- tag_info[!is.na(Release_Date) & !is.na(Release_Time), ]

# make release date column as POSIXct
tag_info[, Release_Date := as.POSIXct(paste(Release_Date,
  Release_Time,
  sep = " "
),
format = "%d.%m.%y %H:%M", tz = "CET"
)]

# sub for knots in data
data_files <- data_files[as.integer(data_ids) %in% tag_info$Toa_Tag]

# map read in, cleaning, and write out function over vector of filenames
invisible(
  lapply(data_files, function(df) {

    # read in the data
    temp_data <- fread(df, integer64 = "numeric")

    # filter for release date + 24 hrs
    temp_id <- str_sub(temp_data[1, TAG], -3, -1)

    rel_date <- tag_info[Toa_Tag == temp_id, Release_Date]

    temp_data <- temp_data[TIME / 1e3 > as.numeric(rel_date + (24 * 3600)), ]
    # do a try catch so as not to break the process
    tryCatch(
      {
        temp_data <- wat_rm_attractor(
          df = temp_data,
          atp_xmin = atp$xmin,
          atp_xmax = atp$xmax,
          atp_ymin = atp$ymin,
          atp_ymax = atp$ymax
        )

        clean_data <- wat_clean_data(
          data = temp_data,
          moving_window = 3,
          nbs_min = 0,
          sd_threshold = 100,
          filter_speed = TRUE,
          speed_cutoff = 150
        )

        agg_data <- wat_agg_data(
          data = clean_data,
          interval = 30
        )

        message(glue("tag {unique(agg_data$id)} \\
                     cleaned with {nrow(agg_data)} fixes"))

        fwrite(
          x = agg_data,
          file = glue("data/data_2018/data_processed/id_{temp_id}.csv"),
          dateTimeAs = "ISO"
        )
        rm(temp_data, clean_data, agg_data)
      },
      error = function(e) {
        message(glue("tag {unique(temp_id)} failed"))
      }
    )
  })
)
```

## Tracking period for each id

```{r}
data_files <- list.files("data/data_2018/data_processed",
  full.names = TRUE
)

total_tracking <- lapply(data_files, function(x) {
  a <- fread(x)
  a <- a[c(1, nrow(a)), list(id, time)]
  a[, event := c("time_start", "time_end")]
})

# bind data tables
total_tracking <- rbindlist(total_tracking)

# get tracking interval for each
total_tracking <- dcast(total_tracking,
  id ~ event,
  value.var = "time"
)

# write to file
fwrite(total_tracking, file = "data/data_2018/data_2018_id_tracking_interval.csv")
```


<!--chapter:end:02_cleaningData.Rmd-->

---
editor_options: 
  chunk_output_type: console
---
```{r include=FALSE, cache=FALSE}
knitr::opts_knit$set(root.dir = here::here())
set.seed(1)
options(digits = 3)

knitr::opts_chunk$set(
  comment = "#>",
  collapse = TRUE,
  eval = FALSE,
  cache = FALSE,
  warning = FALSE,
  error = FALSE,
  message = FALSE,
  out.width = "\\textwidth",
  fig.align = "center",
  fig.width = 7,
  fig.asp = 0.618, # 1 / phi
  fig.show = "hold"
)
options(knitr.kable.NA = "")
options(dplyr.print_min = 6, dplyr.print_max = 6)
```

# Adding tidal cycle data

This section is about adding tidal cycle data to individual trajectories. This is done to split the data up into convenient, and biologically sensible units. This section uses the package `VulnToolkit` [@VulnToolkit2014] to identify high tide times from water-level data provided by Rijkswaterstaat for the measuring point at West Terschelling.

**Workflow**

1. Prepare required libraries,
2. Read in water level data and identify high tides,
3. Write tidal cycle data to local file,
4. Add time since high tide to movement data.

## Prepare libraries

```{r install_vulntoolkit_2, message=FALSE, warning=FALSE}
# load VulnToolkit or install if not available
if ("VulnToolkit" %in% installed.packages() == FALSE) {
  devtools::install_github("troyhill/VulnToolkit")
}
library(VulnToolkit)

# libraries to process data
library(data.table)
library(purrr)
library(glue)
library(dplyr)
library(stringr)
library(fasttime)
library(lubridate)

library(watlastools)
```

## Read water level data

Water level data for [West Terschelling](https://waterinfo.rws.nl/#!/details/publiek/waterhoogte-t-o-v-nap/West-Terschelling(WTER)/Waterhoogte___20Oppervlaktewater___20t.o.v.___20Normaal___20Amsterdams___20Peil___20in___20cm), a settlement approx. 10km from the field site are provided by Rijkswaterstaat's [Waterinfo](https://waterinfo.rws.nl/#!/nav/bulkdownload/parameters/Waterhoogten/), in cm above Amsterdam Ordnance Datum. These data are manually downloaded in the range July 1, 2018 -- October 31, 2018 and saved in `data/data_2018`.

```{r read_waterlevel_data, message=FALSE, warning=FALSE}
# read in waterlevel data
waterlevel <- fread("data/data_2018/waterlevelWestTerschelling.csv", sep = ";")

# select useful columns and rename
waterlevel <- waterlevel[, .(WAARNEMINGDATUM, WAARNEMINGTIJD, NUMERIEKEWAARDE)]

setnames(waterlevel, c("date", "time", "level"))

# make a single POSIXct column of datetime
waterlevel[, dateTime := as.POSIXct(paste(date, time, sep = " "),
  format = "%d-%m-%Y %H:%M:%S", tz = "CET"
)]

waterlevel <- setDT(distinct(setDF(waterlevel), dateTime, .keep_all = TRUE))
```

## Calculate high tides

A tidal period of 12 hours 25 minutes is taken from [Rijkswaterstaat](https://www.rijkswaterstaat.nl/water/waterdata-en-waterberichtgeving/waterdata/getij/index.aspx).

```{r get_high_tide, message=FALSE, warning=FALSE}
# use the HL function from vulnToolkit to get high tides
tides <- VulnToolkit::HL(waterlevel$level,
  waterlevel$dateTime,
  period = 12.41,
  tides = "H", semidiurnal = TRUE
)

# read in release data and get first release - 24 hrs
tag_info <- fread("data/data_2018/SelinDB.csv")
tag_info <- tag_info[!is.na(Release_Date) & !is.na(Release_Time), ]
tag_info[, Release_Date := as.POSIXct(paste(Release_Date,
  Release_Time,
  sep = " "
),
format = "%d.%m.%y %H:%M", tz = "CET"
)]

# remove NAs
tag_info <- na.omit(tag_info, cols = "Release_Date")

first_release <- min(tag_info$Release_Date, na.rm = TRUE) - (3600 * 24)

# remove tides before first release
tides <- setDT(tides)[time > first_release, ][, tide2 := NULL]
tides[, tide_number := seq(nrow(tides))]
```

```{r write_tide_data, message=FALSE, warning=FALSE}
# write to local file
fwrite(tides,
  file = "data/data_2018/tides_2018.csv",
  dateTimeAs = "ISO"
)
```

## Add time since high tide

```{r time_to_high_tide, message=FALSE, warning=FALSE}
# read in data and add time since high tide
data_files <- list.files(
  path = "data/data_2018/data_processed/",
  pattern = "id_", full.names = TRUE
)
data_ids <- str_extract(data_files, "(id_\\d+)") %>% str_sub(-3, -1)

# map read in and tidal time calculation over data
# merge data to insert high tides within movement data
# arrange by time to position high tides correctly
invisible(
  lapply(data_files, function(df) {

    # read and fix data types
    temp_data <- fread(df, integer64 = "numeric")
    temp_data[, ts := fastPOSIXct(ts, tz = "CET")]

    # merge with tides and order on time
    temp_data <- wat_add_tide(
      data = temp_data,
      tide_data = "data/data_2018/tides_2018.csv"
    )

    # add waterlevel
    temp_data[, temp_time := lubridate::round_date(ts, unit = "10 minute")]
    temp_data <- merge(temp_data, waterlevel[, .(dateTime, level)],
      by.x = "temp_time", by.y = "dateTime"
    )
    setnames(temp_data, old = "level", new = "waterlevel")

    # export data, print msg, remove data
    fwrite(temp_data, file = df, dateTimeAs = "ISO")
    message(glue("tag {unique(temp_data$id)} added time since high tide"))
    rm(temp_data)
  })
)
```


<!--chapter:end:03_tidal_cycle.Rmd-->

---
editor_options: 
  chunk_output_type: console
---
```{r include=FALSE, cache=FALSE}
knitr::opts_knit$set(root.dir = here::here())
set.seed(1)
options(digits = 3)

knitr::opts_chunk$set(
  comment = "#>",
  collapse = TRUE,
  eval = FALSE,
  cache = FALSE,
  warning = FALSE,
  error = FALSE,
  message = FALSE,
  out.width = "\\textwidth",
  fig.align = "center",
  fig.width = 7,
  fig.asp = 0.618, # 1 / phi
  fig.show = "hold"
)
options(knitr.kable.NA = "")
options(dplyr.print_min = 6, dplyr.print_max = 6)
```

# Revisit analysis

This section is about splitting the data by tidal cycle, and passing the individual- and tidal cycle-specific data to revisit analysis, which is implemented using the package `recurse`.

**Workflow**

1. Prepare required libraries,
2. Performing recurse:
  - Read in movement data and split by tidal cycle,
  - Perform revisit analysis using `recurse`,
  - Write data with revisit metrics to file.

## Prepare libraries

This section uses the recurse package [@bracis2018].

```{r install_recurse, message=FALSE, warning=FALSE}
# load recurse or install if not available
if ("recurse" %in% installed.packages() == FALSE) {
  install.packages("recurse")
}
library(recurse)

# libraries to process data
library(data.table)
library(purrr)
library(glue)
library(dplyr)
library(fasttime)
library(stringr)
```

## Read data, split, recurse, write

```{r recurse_analysis, message=FALSE, warning=FALSE}
# read in data
data_files <- list.files(
  path = "data/data_2018/data_processed/",
  pattern = "id_", full.names = TRUE
)

data_ids <- str_extract(data_files, "(id_\\d+)") %>% str_sub(-3, -1)
# read in release data and get first release - 24 hrs
tag_info <- fread("data/data_2018/SelinDB.csv")
tag_info <- tag_info[!is.na(Release_Date) & !is.na(Release_Time), ]
tag_info[, Release_Date := as.POSIXct(paste(Release_Date,
  Release_Time,
  sep = " "
),
format = "%d.%m.%y %H:%M", tz = "CET"
)]

# prepare recurse data folder
if (!dir.exists("data/data_2018/data_pre_patch")) {
  dir.create("data/data_2018/data_pre_patch")
}
```

```{r prepare_recurse_params}
# prepare recurse in parameters
# radius (m), cutoff (mins)
radius <- 50
timeunits <- "mins"
revisit_cutoff <- 60
```

```{r do_recurse}
# walk read in, splitting, and recurse over individual level data
# remove visits where the bird left for 60 mins, and then returned
# this is regardless of whether after its return it stayed there
# the removal counts the cumulative sum of all (timeSinceLastVisit <= 60)
# thus after the first 60 minute absence, all points are assigned TRUE
# this must be grouped by the coordinate
walk(data_files, function(df) {

  # read in, fix data type, and split
  temp_data <- fread(df, integer64 = "numeric")
  temp_data[, ts := fastPOSIXct(ts, tz = "CET")]
  setDF(temp_data)
  temp_data <- split(temp_data, temp_data$tide_number)

  # map over the tidal cycle level data
  walk(temp_data, function(tempdf) {
    tryCatch(
      {
        # perform the recursion analysis
        df_recurse <- getRecursions(
          x = tempdf[, c("x", "y", "ts", "id")],
          radius = radius,
          timeunits = timeunits, verbose = TRUE
        )

        # extract revisit statistics and calculate residence time
        # and revisits with a 1 hour cutoff
        df_recurse <- setDT(df_recurse[["revisitStats"]])

        df_recurse[, timeSinceLastVisit :=
          ifelse(is.na(timeSinceLastVisit), -Inf, timeSinceLastVisit)]

        df_recurse[, longAbsenceCounter := cumsum(timeSinceLastVisit > 60),
          by = .(coordIdx)
        ]

        df_recurse <- df_recurse[longAbsenceCounter < 1, ]

        df_recurse <- df_recurse[, .(
          resTime = sum(timeInside),
          fpt = first(timeInside),
          revisits = max(visitIdx)
        ),
        by = .(coordIdx, x, y)
        ]

        # prepare and merge existing data with recursion data
        setDT(tempdf)[, coordIdx := 1:nrow(tempdf)]

        tempdf <- merge(tempdf, df_recurse, by = c("x", "y", "coordIdx"))

        setorder(tempdf, ts)

        # write each data frame to file
        fwrite(tempdf,
          file = glue('data/data_2018/data_pre_patch/{unique(tempdf$id)}_\\
                      {str_pad(unique(tempdf$tide_number), width=3, pad="0")}_\\
                      revisit.csv')
        )

        message(glue('recurse {unique(tempdf$id)}_\\
                     {str_pad(unique(tempdf$tide_number), \\
                     width=3, pad="0")} done'))

        rm(tempdf, df_recurse)
      },
      error = function(e) {
        message("some recurses failed")
      }
    )
  })
})
```


<!--chapter:end:04_residence_time.Rmd-->

---
editor_options: 
  chunk_output_type: console
---
```{r include=FALSE, cache=FALSE}
knitr::opts_knit$set(root.dir = here::here())
set.seed(1)
options(digits = 3)

knitr::opts_chunk$set(
  comment = "#>",
  collapse = TRUE,
  eval = FALSE,
  cache = FALSE,
  warning = FALSE,
  error = FALSE,
  message = FALSE,
  out.width = "\\textwidth",
  fig.align = "center",
  fig.width = 7,
  fig.asp = 0.618, # 1 / phi
  fig.show = "hold"
)
options(knitr.kable.NA = "")
options(dplyr.print_min = 6, dplyr.print_max = 6)
```

# Residence patch construction

This section is about using the main `watlastools` functions to infer residence points when data is missing from a movement track, to classify points into residence or travelling, and to construct low-tide residence patches from the residence points. Summary statistics on these spatial outputs are then exported to file for further use.

**Workflow**

1. Prepare `watlastools` and required libraries,
2. Read data, infer residence, classify points, construct patches, repair patches, and write movement data and patch summary to file.

## Prepare libraries

```{r prep_libs_2, message=FALSE, warning=FALSE}
# load watlastools or install if not available
if (!"watlastools" %in% installed.packages()) {
  devtools::install_github("pratikunterwegs/watlastools", ref = "develop")
}
library(watlastools)

# libraries to process data
library(dplyr)
library(data.table)
library(purrr)
library(stringr)
library(glue)
library(readr)
library(fasttime)

# functions for this stage alone
ci <- function(x) {
  qnorm(0.975) * sd(x, na.rm = T) / sqrt((length(x)))
}
```

## Patch construction

```{r remove_old_data}
if (file.exists("data/data_2018/data_2018_patch_summary.csv")) {
  file.remove("data/data_2018/data_2018_patch_summary.csv")
}
```

Process patches. Takes approx. 5 hours for 3 second data.

```{r make_patches, message=FALSE, warning=FALSE}
# make a vector of data files to read
data_files <- list.files(
  path = "data/data_2018/data_pre_patch",
  pattern = "_revisit.csv", full.names = TRUE
)

# get tag ids
data_id <- str_split(data_files, "/") %>%
  map_chr(function(l) l[[4]] %>% str_sub(1, 3))

# make df of tag ids and files
data <- tibble(tag = data_id, data_file = data_files)
data <- split(x = data, f = data$tag) %>%
  map(function(l) l$data_file)

# map inferResidence, classifyPath, and getPatches over data
walk(data, function(df_list) {
  patch_data <- map(df_list, function(l) {

    # read the data file
    temp_data <- fread(l)
    temp_data[, ts := fastPOSIXct(ts)]

    id <- unique(temp_data$id)
    tide_number <- unique(temp_data$tide_number)

    # wrap process in try catch
    tryCatch(
      {
        # watlastools function to infer residence
        temp_data <- wat_infer_residence(
          data = temp_data,
          inf_patch_time_diff = 30,
          inf_patch_spat_diff = 100
        )

        # watlastools function to classify path
        temp_data <- wat_classify_points(
          data = temp_data,
          lim_res_time = 2,
          min_fix_warning = 3
        )

        # watlastools function to get patches
        patch_dt <- wat_make_res_patch(
          data = temp_data,
          buffer_radius = 10,
          lim_spat_indep = 100,
          lim_time_indep = 30,
          lim_rest_indep = 30,
          min_fixes = 3
        )
        # print message
        message(glue("patches {id}_{tide_number} done"))
        return(patch_dt)
      },
      # null error function, with option to collect data on errors
      error = function(e) {
        message(glue::glue("patches {id}_{tide_number} errored"))
      }
    )
  })

  tryCatch(
    {
      # repair high tide patches across an individual's tidal cycles
      repaired_data <- wat_repair_ht_patches(patch_data_list = patch_data)
      # write patch summary data

      if (all(is.data.frame(repaired_data), nrow(repaired_data) > 0)) {
        # watlastools function to get patch data as summary
        patch_summary <- wat_get_patch_summary(
          res_patch_data = repaired_data,
          which_data = "summary"
        )
        fwrite(patch_summary,
          file = "data/data_2018/data_2018_patch_summary.csv",
          append = TRUE
        )

        # we also want the spatial object
        patch_spatial <- wat_get_patch_summary(
          res_patch_data = repaired_data,
          which_data = "spatial"
        )
        sf::st_crs(patch_spatial) <- 32631
      }
      sf::st_write(patch_spatial,
        dsn = "data/data_2018/spatials/patches_2018.gpkg",
        append = TRUE
      )
    },
    error = function(e) {
      message(glue::glue("patch writing errored"))
    }
  )
})
```

<!--chapter:end:05_residence_patches.Rmd-->

---
editor_options: 
  chunk_output_type: console
---
```{r include=FALSE, cache=FALSE}
knitr::opts_knit$set(root.dir = here::here())
set.seed(1)
options(digits = 3)

knitr::opts_chunk$set(
  comment = "#>",
  collapse = TRUE,
  eval = FALSE,
  cache = FALSE,
  warning = FALSE,
  error = FALSE,
  message = FALSE,
  out.width = "\\textwidth",
  fig.align = "center",
  fig.width = 7,
  fig.asp = 0.618, # 1 / phi
  fig.show = "hold"
)
options(knitr.kable.NA = "")
options(dplyr.print_min = 6, dplyr.print_max = 6)
```

# Bathymetry of the Griend mudflats

## Get data and plot basic trend

```{r load_libs_s01}
# load libs to read bathymetry
library(raster)
library(rayshader)
library(sf)

# data libs
library(data.table)
library(purrr)
library(stringr)

# plot libs
library(ggplot2)
library(ggthemes)
```

```{r load_data_s01}
# load bathymetry and subset
data <- raster("data/bathymetry_waddenSea_2015.tif")
griend <- st_read("griend_polygon/griend_polygon.shp") %>% 
  st_buffer(10.5e3)

# assign utm 31 crs and subset
crs(data) <- unlist(st_crs(griend)[2])
data <- crop(data, as(griend, "Spatial"))
data <- aggregate(data, fact = 2)
# remove 20m outliers
#data[data > 500] <- NA
data_m <- raster_to_matrix(data)

# get quantiles of the matrix
data_q <- quantile(data_m, na.rm = T, probs = 1:1000/1000)
```

```{r load_waterlevel_s01}
# read in waterlevel data
waterlevel <- fread("data/data_2018/waterlevelWestTerschelling.csv", sep = ";")

# select useful columns and rename
waterlevel <- waterlevel[,.(WAARNEMINGDATUM, WAARNEMINGTIJD, NUMERIEKEWAARDE)]

setnames(waterlevel, c("date", "time", "level"))

# make a single POSIXct column of datetime
waterlevel[,dateTime := as.POSIXct(paste(date, time, sep = " "), 
                                   format = "%d-%m-%Y %H:%M:%S", tz = "CET")]

waterlevel <- setDT(distinct(setDF(waterlevel), dateTime, .keep_all = TRUE))
```

```{r plot_bathymetry_quantiles}
# plot waterlevel quantiles with data from west terschelling
waterlimits <- range(waterlevel$level)

fig_waterlevel_area <- ggplot()+
  geom_line(aes(x = 1:1000/1000, y = data_q))+
  geom_hline(yintercept = waterlimits, col = "blue", lty = 2)+
  geom_hline(yintercept = 0, lty = 2, col = "red")+
  scale_x_continuous(labels = scales::percent)+
  theme_bw()+
  coord_flip(xlim = c(0.25, 1.01), ylim = c(-250, 250))+
  labs(x = "% area covered", y = "waterlevel (cm over NAP)")

ggsave(fig_waterlevel_area, filename = "figs/fig_waterlevel_area.png",
       dpi = 300, height = 4, width = 6); dev.off()
```

## Plot as 3D maps

```{r make_water_seq}
# make sequence
waterdepth <- seq(waterlimits[1], waterlimits[2], length.out = 30)
waterdepth <- c(waterdepth, rev(waterdepth))
```


```{r vis_data}
# make visualisation
for(i in 1:length(waterdepth)) {
  data_m %>%
    sphere_shade(texture = "imhof1", zscale = 50) %>%
    # add_water(detect_water(data_m, zscale = 100, 
    #                        max_height = waterdepth[i],cutoff = 0.1), 
    #           color = "desert") %>%
    add_shadow(hillshade = data_m) %>%
    plot_3d(data_m, zscale = 75, wateralpha = 0.6,
            solid =F, shadow=F,
            water = TRUE,
            waterdepth = waterdepth[i],
            watercolor = "dodgerblue1",
            phi = 90,
            theta = 0, zoom = 0.75, windowsize = c(1000, 800),
            background = "black", calculate_normals = F)
  render_label(data_m, x = 500, y = 500, z = 500,
               text = glue::glue('waterdepth = {round(waterdepth[i])} cm'), 
               freetype = F, textcolor = "black")
    
  rgl::snapshot3d(paste0("figs/tide_rise/fig",str_pad(i, 2, pad = "0"),".png"))
  rgl::rgl.close()
}
```

```{r make_tide_gif}
library(magick)
list.files(path = "figs/tide_rise/", pattern = "*.png", full.names = T) %>% 
  map(image_read) %>% # reads each path file
  image_join() %>% # joins image
  image_animate(fps=2) %>% # animates, can opt for number of loops
  image_write("figs/fig_tide_rise_anim.gif") # write to current dir
```


```{r include_gif, eval=TRUE, fig.cap="Waterlevel at West Terschelling and effect on coverage of mudflats around Griend."}
if (knitr:::is_latex_output()) {
  
} else {
  knitr::include_graphics("figs/fig_tide_rise_anim.gif")
}
```


<!--chapter:end:21_supplement01_waterlevelGriend.rmd-->

---
editor_options: 
  chunk_output_type: console
---
```{r include=FALSE, cache=FALSE}
knitr::opts_knit$set(root.dir = here::here())
set.seed(1)
options(digits = 3)

knitr::opts_chunk$set(
  comment = "#>",
  collapse = TRUE,
  eval = FALSE,
  cache = FALSE,
  warning = FALSE,
  error = FALSE,
  message = FALSE,
  out.width = "\\textwidth",
  fig.align = "center",
  fig.width = 7,
  fig.asp = 0.618, # 1 / phi
  fig.show = "hold"
)
options(knitr.kable.NA = "")
options(dplyr.print_min = 6, dplyr.print_max = 6)
```

# Residence patch cleaning

Residence patches must be cleaned to remove patches where the speed between patches is too high, or too low. We filter for $0 < v < 150$, where $v$ is the speed between patches.

We also filter out patches that are too small, ie, fewer than 10 fixes (approx. 5 minutes).

Once cleaned, these patches can be passed on to the overlap finding algorithm.

## Prepare libraries

```{r prep_libs, message=FALSE, warning=FALSE}
library(data.table)
library(purrr)
```

## Load patches

```{r}
# load data
patches <- fread("data/data_2018/data_2018_patch_summary.csv")

# add uid
patches[, uid := seq_len(nrow(patches))]
```

## Calculate speed between patches

```{r}
# this is the speed in metres per second
patches[, speed := distBwPatch / 
          c(NA, time_start[-1] - 
              time_end[seq_len(length(time_end) - 1)]),
        by = .(id, tide_number)]

# what is 150 km/hr in m/s
cutoff_speed <- 15 # around 54 kmph

# filter ridiculous speeds
patches <- patches[between(speed, 1e-4, cutoff_speed), ]
```

## Remove sequences of reflected patches

```{r}
# recalculate the distance b/w patches
patches2 <- patches[id == "554", ]
patches2 <- patches2[duration >= 10 * 60, ]

patches2 <- split(patches2, by = c("id"))


# this is the speed in metres per second
patches2 <- map(patches2, function(df) {
  
  df <- df[speed < 15, ]
  
  df[, distBwPatch := watlastools::wat_bw_patch_dist(df)]
  
  # replace 
  df[, distBwPatch := nafill(distBwPatch, type = "const", fill = Inf)]
  
  df[, speed := distBwPatch / 
          c(1, time_start[-1] - 
              time_end[seq_len(length(time_end) - 1)])]
  
  # where is the speed greater than a cutoff
  df[, reflection := cumsum(speed > cutoff_speed)]
  
  return(df)
})
```

```{r}
# bind the rows
patches2 <- rbindlist(patches2)
patches2[tide_number == 96, ]
```


The number of patches is down to 57,089.

## Export this 'good' patch data

```{r}
# export the summary
fwrite(patches, "data/data_2018/data_2018_good_patches.csv")
```

<!--chapter:end:xx_clean_residence_patches.Rmd-->

