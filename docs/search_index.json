[
["index.html", "Individual space-use in relation to the social environment and resource landscape Section 1 Introduction 1.1 Attribution 1.2 Data access 1.3 Data processing", " Individual space-use in relation to the social environment and resource landscape Pratik R Gupte 2020-05-04 Section 1 Introduction This is the bookdown version of a project in preparation that identifies residence patches from high-resolution tracking data from individual red knots Calidris canutus and examines the modulation of individual behaviour by the social environment and the resource landscape. 1.1 Attribution Please contact the following before cloning or in case of interest in the project. Pratik Gupte (author and maintainer) PhD student, GELIFES – University of Groningen Guest researcher, COS – NIOZ p.r.gupte@rug.nl Nijenborgh 7/5172.0583 9747AG Groningen 1.2 Data access The data used in this work are not publicly available. Contact PI Allert Bijleveld (COS-NIOZ) for data access. 1.3 Data processing The data processing for this project is described in the following sections. Navigate through them using the links in the sidebar. "],
["getting-data.html", "Section 2 Getting data 2.1 Prepare watlastools and other libraries 2.2 Read in tag deployment data 2.3 Get data and save locally", " Section 2 Getting data This section focusses on accessing and downloading WATLAS data. This is done using functions in the WATLAS Utilities package. Workflow Preparing required libraries. Reading tag data with deployment start dates from a local file. This file is not yet publicly available. Connecting to the NIOZ databse and downloading data. This database is also not public-access. 2.1 Prepare watlastools and other libraries # install the package watlastools from master branch using the following # install.packages(&quot;devtools&quot;) library(devtools) # devtools::install_github(&quot;pratikunterwegs/watlastools&quot;) library(watlastools) # libraries to process data library(data.table) library(ggplot2) library(ggthemes) library(purrr) library(glue) 2.2 Read in tag deployment data # read deployment data from local file in data folder tag_info &lt;- fread(&quot;data/data2018/SelinDB.csv&quot;) # filter out NAs in release date and time tag_info &lt;- tag_info[!is.na(Release_Date) &amp; !is.na(Release_Time),] # make release date column as POSIXct tag_info[,Release_Date := as.POSIXct(paste(Release_Date, Release_Time, sep = &quot; &quot;), format = &quot;%d.%m.%y %H:%M&quot;, tz = &quot;CET&quot;)] # check new release date column head(tag_info$Release_Date) ## [1] &quot;2018-09-14 20:00:00 CEST&quot; &quot;2018-09-13 15:30:00 CEST&quot; ## [3] &quot;2018-09-14 20:00:00 CEST&quot; &quot;2018-09-13 18:30:00 CEST&quot; ## [5] &quot;2018-08-11 15:00:00 CEST&quot; &quot;2018-08-16 18:04:00 CEST&quot; (#fig:plot_release_schedule)Knots released per week of 2018. 2.3 Get data and save locally # read in database access parameters from a local file data_access &lt;- fread(&quot;data/access_params.txt&quot;) # create a data storage file if not present # use the getData function from watlastools on the tag_info data frame # this is placed inside a pmap wrapper to automate access for all birds if(!dir.exists(&quot;data/data2018&quot;)) { dir.create(&quot;data/data2018&quot;) } pmap(tag_info[,.(Toa_Tag, Release_Date)], function(Toa_Tag, Release_Date){ prelim_data &lt;- watlastools::wat_get_data(tag = Toa_Tag, tracking_time_start = as.character(Release_Date), tracking_time_end = &quot;2018-10-31&quot;, username = data_access$username, password = data_access$password) setDT(prelim_data) # prelim_data[,TAG:= = as.numeric(TAG) - prefix_num] message(glue(&#39;tag {Toa_Tag} accessed with {nrow(prelim_data)} fixes&#39;)) fwrite(prelim_data, file = glue(&#39;data/data2018/{Toa_Tag}_data.csv&#39;), dateTimeAs = &quot;ISO&quot;) }) "],
["cleaning-data.html", "Section 3 Cleaning data 3.1 Prepare watlastools and other libraries 3.2 Prepare to remove attractor points 3.3 Read, clean, and write data", " Section 3 Cleaning data This section is about cleaning downloaded data using the cleanData function in the WATLAS Utilities package. Workflow Prepare required libraries. Read in data, apply the cleaning function, and overwrite local data. 3.1 Prepare watlastools and other libraries # watlastools assumed installed from the previous step # if not, install from the github repo as shown below devtools::install_github(&quot;pratikunterwegs/watlastools&quot;) library(watlastools) # libraries to process data library(data.table) library(purrr) library(glue) library(fasttime) library(bit64) library(stringr) 3.2 Prepare to remove attractor points # read in identified attractor points atp &lt;- fread(&quot;data/data2018/attractor_points.txt&quot;) 3.3 Read, clean, and write data # make a list of data files to read data_files &lt;- list.files(path = &quot;data/data2018/locs_raw&quot;, pattern = &quot;whole_season*&quot;, full.names = TRUE) data_ids &lt;- str_extract(data_files, &quot;(tx_\\\\d+)&quot;) %&gt;% str_sub(-3,-1) # read deployment data from local file in data folder tag_info &lt;- fread(&quot;data/data2018/SelinDB.csv&quot;) # filter out NAs in release date and time tag_info &lt;- tag_info[!is.na(Release_Date) &amp; !is.na(Release_Time),] # make release date column as POSIXct tag_info[,Release_Date := as.POSIXct(paste(Release_Date, Release_Time, sep = &quot; &quot;), format = &quot;%d.%m.%y %H:%M&quot;, tz = &quot;CET&quot;)] # sub for knots in data data_files &lt;- data_files[as.integer(data_ids) %in% tag_info$Toa_Tag] # map read in, cleaning, and write out function over vector of filenames map(data_files, function(df){ temp_data &lt;- fread(df, integer64 = &quot;numeric&quot;) # filter for release date + 24 hrs { temp_id &lt;- str_sub(temp_data[1, TAG], -3, -1) rel_date &lt;- tag_info[Toa_Tag == temp_id, Release_Date] temp_data &lt;- temp_data[TIME/1e3 &gt; as.numeric(rel_date + (24*3600)),] } tryCatch( { temp_data &lt;- wat_rm_attractor(df = temp_data, atp_xmin = atp$xmin, atp_xmax = atp$xmax, atp_ymin = atp$ymin, atp_ymax = atp$ymax) clean_data &lt;- wat_clean_data(somedata = temp_data, moving_window = 3, nbs_min = 0, sd_threshold = 100, filter_speed = TRUE, speed_cutoff = 150) agg_data &lt;- wat_agg_data(df = clean_data, interval = 30) message(glue(&#39;tag {unique(agg_data$id)} cleaned with {nrow(agg_data)} fixes&#39;)) fwrite(x = agg_data, file = glue(&#39;data/data2018/locs_proc/id_{temp_id}.csv&#39;), dateTimeAs = &quot;ISO&quot;) rm(temp_data, clean_data, agg_data) }, error = function(e){ message(glue(&#39;tag {unique(temp_id)} failed&#39;)) }) }) "],
["adding-tidal-cycle-data.html", "Section 4 Adding tidal cycle data 4.1 Prepare libraries 4.2 Read water level data 4.3 Calculate high tides 4.4 Add time since high tide", " Section 4 Adding tidal cycle data This section is about adding tidal cycle data to individual trajectories. This is done to split the data up into convenient, and biologically sensible units. This section uses the package VulnToolkit (Troy D. Hill, Shimon C. Anisfeld 2014) to identify high tide times from water-level data provided by Rijkswaterstaat for the measuring point at West Terschelling. Workflow Prepare required libraries, Read in water level data and identify high tides, Write tidal cycle data to local file, Add time since high tide to movement data. 4.1 Prepare libraries # load VulnToolkit or install if not available if(&quot;VulnToolkit&quot; %in% installed.packages() == FALSE){ devtools::install_github(&quot;troyhill/VulnToolkit&quot;) } library(VulnToolkit) # libraries to process data library(data.table) library(purrr) library(glue) library(dplyr) library(stringr) library(fasttime) library(lubridate) library(watlastools) 4.2 Read water level data Water level data for West Terschelling, a settlement approx. 10km from the field site are provided by Rijkswaterstaat’s Waterinfo, in cm above Amsterdam Ordnance Datum. These data are manually downloaded in the range July 1, 2018 – October 31, 2018 and saved in data/data2018. # read in waterlevel data waterlevel &lt;- fread(&quot;data/data2018/waterlevelWestTerschelling.csv&quot;, sep = &quot;;&quot;) # select useful columns and rename waterlevel &lt;- waterlevel[,.(WAARNEMINGDATUM, WAARNEMINGTIJD, NUMERIEKEWAARDE)] setnames(waterlevel, c(&quot;date&quot;, &quot;time&quot;, &quot;level&quot;)) # make a single POSIXct column of datetime waterlevel[,dateTime := as.POSIXct(paste(date, time, sep = &quot; &quot;), format = &quot;%d-%m-%Y %H:%M:%S&quot;, tz = &quot;CET&quot;)] waterlevel &lt;- setDT(distinct(setDF(waterlevel), dateTime, .keep_all = TRUE)) 4.3 Calculate high tides A tidal period of 12 hours 25 minutes is taken from Rijkswaterstaat. # use the HL function from vulnToolkit to get high tides tides &lt;- VulnToolkit::HL(waterlevel$level, waterlevel$dateTime, period = 12.41, tides = &quot;H&quot;, semidiurnal = TRUE) # read in release data and get first release - 24 hrs tag_info &lt;- fread(&quot;data/data2018/SelinDB.csv&quot;) tag_info &lt;- tag_info[!is.na(Release_Date) &amp; !is.na(Release_Time),] tag_info[,Release_Date := as.POSIXct(paste(Release_Date, Release_Time, sep = &quot; &quot;), format = &quot;%d.%m.%y %H:%M&quot;, tz = &quot;CET&quot;)] first_release &lt;- min(tag_info$Release_Date) - (3600*24) # remove tides before first release tides &lt;- setDT(tides)[time &gt; first_release, ][,tide2:=NULL] tides[,tide_number:=1:nrow(tides)] # write to local file fwrite(tides, file = &quot;data/data2018/tidesSummer2018.csv&quot;, dateTimeAs = &quot;ISO&quot;) 4.4 Add time since high tide # read in data and add time since high tide data_files &lt;- list.files(path = &quot;data/data2018/locs_proc&quot;, pattern = &quot;id_&quot;, full.names = TRUE) data_ids &lt;- str_extract(data_files, &quot;(id_\\\\d+)&quot;) %&gt;% str_sub(-3,-1) # map read in and tidal time calculation over data # merge data to insert high tides within movement data # arrange by time to position high tides correctly map(data_files, function(df){ # read and fix data types temp_data &lt;- fread(df, integer64 = &quot;numeric&quot;) temp_data[,ts:=fastPOSIXct(ts, tz = &quot;CET&quot;)] # merge with tides and order on time temp_data &lt;- wat_add_tide(df = temp_data, tide_data = &quot;data/data2018/tidesSummer2018.csv&quot;) # add waterlevel temp_data[,temp_time := lubridate::round_date(ts, unit = &quot;10 minute&quot;)] temp_data &lt;- merge(temp_data, waterlevel[,.(dateTime, level)], by.x = &quot;temp_time&quot;, by.y = &quot;dateTime&quot;) setnames(temp_data, old = &quot;level&quot;, new = &quot;waterlevel&quot;) # export data, print msg, remove data fwrite(temp_data, file = df, dateTimeAs = &quot;ISO&quot;) message(glue(&#39;tag {unique(temp_data$id)} added time since high tide&#39;)) rm(temp_data) }) References "],
["revisit-analysis.html", "Section 5 Revisit analysis 5.1 Prepare libraries 5.2 Read data, split, recurse, write", " Section 5 Revisit analysis This section is about splitting the data by tidal cycle, and passing the individual- and tidal cycle-specific data to revisit analysis, which is implemented using the package recurse. Workflow Prepare required libraries, Performing recurse: Read in movement data and split by tidal cycle, Perform revisit analysis using recurse, Write data with revisit metrics to file. 5.1 Prepare libraries This section uses the recurse package (Bracis, Bildstein, and Mueller 2018). # load recurse or install if not available if(&quot;recurse&quot; %in% installed.packages() == FALSE){ install.packages(&quot;recurse&quot;) } library(recurse) # libraries to process data library(data.table) library(purrr) library(glue) library(dplyr) library(fasttime) library(stringr) 5.2 Read data, split, recurse, write # read in data data_files &lt;- list.files(path = &quot;data/data2018/&quot;, pattern = &quot;id_&quot;, full.names = TRUE) data_ids &lt;- str_extract(data_files, &quot;(id_\\\\d+)&quot;) %&gt;% str_sub(-3,-1) { # read in release data and get first release - 24 hrs tag_info &lt;- fread(&quot;data/data2018/SelinDB.csv&quot;) tag_info &lt;- tag_info[!is.na(Release_Date) &amp; !is.na(Release_Time),] tag_info[,Release_Date := as.POSIXct(paste(Release_Date, Release_Time, sep = &quot; &quot;), format = &quot;%d.%m.%y %H:%M&quot;, tz = &quot;CET&quot;)] } # prepare recurse data folder if(!dir.exists(&quot;data/data2018/revisitData&quot;)){ dir.create(&quot;data/data2018/revisitData&quot;) } # prepare recurse in parameters # radius (m), cutoff (mins) { radius &lt;- 50 timeunits &lt;- &quot;mins&quot; revisit_cutoff &lt;- 60 } # map read in, splitting, and recurse over individual level data # remove visits where the bird left for 60 mins, and then returned # this is regardless of whether after its return it stayed there # the removal counts the cumulative sum of all (timeSinceLastVisit &lt;= 60) # thus after the first 60 minute absence, all points are assigned TRUE # this must be grouped by the coordinate map(data_files, function(df){ # read in, fix data type, and split temp_data &lt;- fread(df, integer64 = &quot;numeric&quot;) temp_data[,ts:=fastPOSIXct(ts, tz = &#39;CET&#39;)] setDF(temp_data) temp_data &lt;- split(temp_data, temp_data$tide_number) # map over the tidal cycle level data map(temp_data, function(tempdf){ tryCatch({ # perform the recursion analysis df_recurse &lt;- getRecursions(x = tempdf[,c(&quot;x&quot;,&quot;y&quot;,&quot;ts&quot;,&quot;id&quot;)], radius = radius, timeunits = timeunits, verbose = TRUE) # extract revisit statistics and calculate residence time # and revisits with a 1 hour cutoff df_recurse &lt;- setDT(df_recurse[[&quot;revisitStats&quot;]]) df_recurse[,timeSinceLastVisit:= ifelse(is.na(timeSinceLastVisit), -Inf, timeSinceLastVisit)] df_recurse[,longAbsenceCounter:= cumsum(timeSinceLastVisit &gt; 60), by= .(coordIdx)] df_recurse &lt;- df_recurse[longAbsenceCounter &lt; 1,] df_recurse &lt;- df_recurse[,.(resTime = sum(timeInside), fpt = first(timeInside), revisits = max(visitIdx)), by=.(coordIdx,x,y)] # prepare and merge existing data with recursion data setDT(tempdf)[,coordIdx:=1:nrow(tempdf)] tempdf &lt;- merge(tempdf, df_recurse, by = c(&quot;x&quot;, &quot;y&quot;, &quot;coordIdx&quot;)) setorder(tempdf, ts) # write each data frame to file fwrite(tempdf, file = glue(&#39;data/data2018/revisitData/{unique(tempdf$id)}_{str_pad(unique(tempdf$tide_number), width=3, pad=&quot;0&quot;)}_revisit.csv&#39;)) message(glue(&#39;recurse {unique(tempdf$id)}_{str_pad(unique(tempdf$tide_number), width=3, pad=&quot;0&quot;)} done&#39;)) rm(tempdf, df_recurse) }, error = function(e){ message(&quot;some recurses failed&quot;) }) }) }) References "],
["residence-patch-construction.html", "Section 6 Residence patch construction 6.1 Prepare libraries 6.2 Patch construction", " Section 6 Residence patch construction This section is about using the main watlastools functions to infer residence points when data is missing from a movement track, to classify points into residence or travelling, and to construct low-tide residence patches from the residence points. Summary statistics on these spatial outputs are then exported to file for further use. Workflow Prepare watlastools and required libraries, Read data, infer residence, classify points, construct patches, repair patches, and write movement data and patch summary to file. 6.1 Prepare libraries # load watlastools or install if not available if(&quot;watlastools&quot; %in% installed.packages() == FALSE){ devtools::install_github(&quot;pratikunterwegs/watlastools&quot;) } library(watlastools) # libraries to process data library(dplyr) library(data.table) library(purrr) library(stringr) library(glue) library(readr) library(fasttime) # functions for this stage alone ci &lt;- function(x){qnorm(0.975)*sd(x, na.rm = T)/sqrt((length(x)))} 6.2 Patch construction if(file.exists(&quot;data/data2018/data_2018_tidal_mean_speed.csv&quot;)){ file.remove(&quot;data/data2018/data_2018_tidal_mean_speed.csv&quot;) } if(file.exists(&quot;data/data2018/data_2018_patch_summary.csv&quot;)){ file.remove(&quot;data/data2018/data_2018_patch_summary.csv&quot;) } Process patches. Takes approx. 5 hours for 3 second data. # make a vector of data files to read data_files &lt;- list.files(path = &quot;data/data2018/revisitData&quot;, pattern = &quot;_revisit.csv&quot;, full.names = TRUE) # get tag ids data_id &lt;- str_split(data_files, &quot;/&quot;) %&gt;% map(function(l)l[[4]] %&gt;% str_sub(1,3)) %&gt;% flatten_chr() # make df of tag ids and files data &lt;- data_frame(tag = data_id, data_file = data_files) data &lt;- split(x = data, f = data$tag) %&gt;% map(function(l) l$data_file) # map inferResidence, classifyPath, and getPatches over data for(i in 1:length(data)){ patch_data &lt;- map(data[[i]], function(l){ # read the data file temp_data &lt;- fread(l) temp_data[,ts:=fastPOSIXct(ts)] id &lt;- unique(temp_data$id) tide_number &lt;- unique(temp_data$tide_number) # get data summary for the tidal cycle { data_summary &lt;- temp_data[,.(duration = (max(time) - min(time))/60, n_fixes = length(x), prop_fixes = length(x) / ((max(time) - min(time))/30)), by = .(id, tide_number)] sld &lt;- watlastools::wat_simple_dist(temp_data, &quot;x&quot;, &quot;y&quot;) timelag &lt;- c(NA, as.numeric(diff(temp_data$time))) speed &lt;- sld/timelag data_summary[,`:=`(mean_speed = mean(speed, na.rm=TRUE), ci95_speed = ci(speed))] # write total distance for the tidal cycle fwrite(data_summary, file = &quot;data/data2018/data_2018_tidal_mean_speed.csv&quot;, append = TRUE, scipen = 6) } # wrap process in try catch tryCatch( { # watlastools function to infer residence temp_data &lt;- wat_infer_residence(df = temp_data, infPatchTimeDiff = 30, infPatchSpatDiff = 100) # watlastools function to classify path temp_data &lt;- wat_classify_points(somedata = temp_data, resTimeLimit = 2) # watlastools function to get patches patch_dt &lt;- wat_make_res_patch(somedata = temp_data, bufferSize = 10, spatIndepLim = 100, tempIndepLim = 30, restIndepLim = 30, minFixes = 3) # print message message(as.character(glue(&#39;patches {id}_{tide_number} done&#39;))) return(patch_dt) }, # null error function, with option to collect data on errors error= function(e) { message(glue::glue(&#39;patches {id}_{tide_number} errored&#39;)) } ) }) # repair high tide patches across an individual&#39;s tidal cycles repaired_data &lt;- wat_repair_ht_patches(patch_data_list = patch_data) # write patch summary data if(all(is.data.frame(repaired_data), nrow(repaired_data) &gt; 0)){ # watlastools function to get patch data as spatial patch_summary &lt;- wat_get_patch_summary(resPatchData = repaired_data, whichData = &quot;summary&quot;) fwrite(patch_summary, file = &quot;data/data2018/data_2018_patch_summary.csv&quot;, append = TRUE) } } "],
["finding-spatio-temporal-patch-clusters.html", "Section 7 Finding spatio-temporal patch clusters 7.1 Load libs 7.2 Prepare patch data 7.3 Prepare Python libraries 7.4 Find patch clusters", " Section 7 Finding spatio-temporal patch clusters 7.1 Load libs # for data library(tidyverse) # python options library(reticulate) # set python path use_python(&quot;/usr/bin/python3&quot;) 7.2 Prepare patch data Load data and filter for quality. Keep tides with at least 10 individuals, and inviduals recorded in at least 10 tides. # get data and filter for individuals represented in 10 tides # and tides with 10 or more individuals data &lt;- read_csv(&quot;data/data_2018_patch_summary.csv&quot;) good_tides &lt;- group_by(data, tide_number) %&gt;% summarise(birds_in_tide = length(unique(id))) %&gt;% filter(birds_in_tide &gt;= 10) good_birds &lt;- group_by(data, id) %&gt;% summarise(tides_present = length(unique(tide_number))) %&gt;% filter(tides_present &gt;= 10) # now filter the data data &lt;- filter(data, tide_number %in% good_tides$tide_number, id %in% good_birds$id) # write good patches write_csv(data, &quot;data/data_2018_good_patches.csv&quot;) 7.3 Prepare Python libraries # network lib import networkx as nx import numpy as np import pandas as pd # import ckdtree from scipy.spatial import cKDTree def round_any(value, limit): return round(value/limit)*limit # function to use ckdtrees for nearest point finding def make_patch_pairs(patch_data, dist_indep): coords = patch_data[[&#39;x_mean&#39;, &#39;y_mean&#39;]] coords = np.asarray(coords) ckd_tree = cKDTree(coords) pairs = ckd_tree.query_pairs(r=dist_indep, output_type=&#39;ndarray&#39;) return pairs # make modules from patch data # function to process ckd_pairs def make_patch_modules(patch_data, scale): # assign a unique id per dataframe patch_data[&#39;within_tide_id&#39;] = np.arange(len(patch_data)) patch_pairs = make_patch_pairs(patch_data=patch_data, dist_indep=scale) if len(patch_pairs) &gt; 1: patch_pairs = pd.DataFrame(data=patch_pairs, columns=[&#39;p1&#39;, &#39;p2&#39;]) # get unique patch ids unique_patches = np.concatenate((patch_pairs.p1.unique(), patch_pairs.p2.unique())) unique_patches = np.unique(unique_patches) # make network network = nx.from_pandas_edgelist(patch_pairs, &#39;p1&#39;, &#39;p2&#39;) # get modules modules = list(nx.algorithms.community.greedy_modularity_communities(network)) # get modules as df m = [] for i in np.arange(len(modules)): module_number = [i] * len(modules[i]) module_coords = list(modules[i]) m = m + list(zip(module_number, module_coords)) # add location, bird and tide aux_data = patch_data[patch_data.within_tide_id.isin(unique_patches)][ [&#39;id&#39;, &#39;tide_number&#39;, &#39;patch&#39;, &#39;within_tide_id&#39;, &#39;time_scale&#39;, &#39;time_chunk&#39;]] module_data = pd.DataFrame(m, columns=[&#39;module&#39;, &#39;within_tide_id&#39;]) module_data = pd.merge(module_data, aux_data, on=&#39;within_tide_id&#39;) # add scale module_data[&#39;spatial_scale&#39;] = scale return module_data else: return None Send the list of data frames to Python. # send to python py$data_py = data 7.4 Find patch clusters # run code import os import pandas as pd import numpy as np import itertools from helper_functions import make_patch_modules, round_any # read in the data data = pd.read_csv(&quot;data/data_2018_good_patches.csv&quot;) # use good_patches for quality control data.head() # look at one tidal cycle, first count patches per tide pd.value_counts(data[&#39;tide_number&#39;]) # choose the highest # data = data[data[&#39;tide_number&#39;] == 73] # assign rounded values of time rather than tide number # do this in a list time_scale = [1, 3, 6, 12] data_list = [] # what is the min of time min_time = data.time_mean.min()/3600 for i in np.arange(len(time_scale)): tmp_data = data tmp_data[&#39;round_time&#39;] = round_any((tmp_data[&#39;time_mean&#39;]/3600) - min_time, time_scale[i]) tmp_data[&#39;time_scale&#39;] = time_scale[i] data_list.append([pd.DataFrame(y) for x, y in tmp_data.groupby(&#39;round_time&#39;, as_index=False)]) # add time chunk for i in np.arange(len(data_list)): for j in np.arange(len(data_list[i])): data_list[i][j][&#39;time_chunk&#39;] = j # flatten this list, time_scale is stored in each df data_list = list(itertools.chain(*data_list)) # remove lists with single patch data_list = [df for df in data_list if len(df) &gt; 1] # now get modules over spatial scales # there are 4 list elements of temporal scale # times 4 spatial scales # run over spatial scales 100, 250, 500, 1000 spatial_scales = [50, 100, 250, 500] ml_list = list(map(lambda x: list(map(make_patch_modules, data_list, [x]*len(data_list))), spatial_scales)) # flatten module list ml_list = list(itertools.chain(*ml_list)) ml_list2 = [i for i in ml_list if i is not None] # concatenate data ml_data = pd.concat(ml_list2) # write to file ml_data.to_csv(index=False, path_or_buf=&quot;data/data_2018_patch_modules_small_scale.csv&quot;) "],
["basic-cluster-description.html", "Section 8 Basic cluster description 8.1 Prepare libraries 8.2 Read in data 8.3 Summarise patches per module 8.4 Summarise birds per module 8.5 Does number of patches scale with individuals in module? 8.6 Module metrics and composition ~ waterlevel 8.7 Visualise modules", " Section 8 Basic cluster description 8.1 Prepare libraries library(tidyverse) # for plots library(ggplot2) library(scico) # ci function ci &lt;- function(x){qnorm(0.975)*sd(x, na.rm = TRUE)/sqrt(length(x))} 8.2 Read in data # read in data modules &lt;- read_csv(&quot;data/data_2018_patch_modules_small_scale.csv&quot;) 8.3 Summarise patches per module # count modules at each scale and plot module_per_scale &lt;- modules %&gt;% group_by(time_scale, spatial_scale, time_chunk) %&gt;% summarise_at(vars(module), list(~length(.))) %&gt;% group_by(time_scale, spatial_scale) %&gt;% summarise(mean_n_modules = mean(module)) fig_modules_scale &lt;- ggplot(module_per_scale)+ geom_tile(aes(factor(time_scale), factor(spatial_scale), fill = mean_n_modules), col = &quot;grey&quot;)+ scale_fill_scico(palette = &quot;nuuk&quot;, # limits = c(NA, 1000), direction = -1, trans = &quot;log10&quot;)+ theme_test()+ theme(legend.key.width = unit(0.1, &quot;cm&quot;))+ labs(x = &quot;time scale (hours)&quot;, y = &quot;spatial scale (m)&quot;, fill = &quot;patches&quot;, title = &quot;(a)&quot;) 8.4 Summarise birds per module How many ids in each scale, on average? # count unique individuals per time chunk per module per scale id_per_scale &lt;- modules %&gt;% group_by(time_scale, spatial_scale, time_chunk, module) %&gt;% summarise(unique_id = length(unique(id))) %&gt;% group_by(time_scale, spatial_scale) %&gt;% summarise(mean_uid = mean(unique_id)) fig_id_scale &lt;- ggplot(id_per_scale)+ geom_tile(aes(factor(time_scale), factor(spatial_scale), fill = mean_uid), col = &quot;grey&quot;)+ scale_fill_scico(palette = &quot;broc&quot;, limits = c(2, NA), na.value = &quot;grey&quot;, direction = 1)+ theme_test()+ theme(legend.key.width = unit(0.1, &quot;cm&quot;))+ labs(x = &quot;time scale (hours)&quot;, y = &quot;spatial scale (m)&quot;, fill = &quot;birds&quot;, title = &quot;(b)&quot;) # make patchwork and export fig_scales &lt;- patchwork::wrap_plots(fig_modules_scale, fig_id_scale) ggsave(fig_scales, filename = &quot;figs/fig_modules_scale_small.png&quot;, dpi = 300, height = 3, width = 7) knitr::include_graphics(&quot;figs/fig_modules_scale_small.png&quot;) 8.5 Does number of patches scale with individuals in module? # get module size at each scale patch_per_mod &lt;- modules %&gt;% group_by(time_scale, spatial_scale, time_chunk, module) %&gt;% summarise(n_patches = length(patch), n_uid = length(unique(id))) fig_patches_id &lt;- ggplot(patch_per_mod)+ geom_abline(slope = c(1,2,3), col = c(&quot;red&quot;), size = 0.1)+ geom_point(aes(n_uid, n_patches), size = 0.1, alpha = 0.2, col = &quot;steelblue&quot;)+ theme_test()+ facet_grid(time_scale~spatial_scale, labeller = label_both, scales = &quot;free_y&quot;, as.table = FALSE, switch = &quot;both&quot;)+ labs(x = &quot;unique individuals&quot;, y = &quot;# patches&quot;) # save figure ggsave(fig_patches_id, filename = &quot;figs/fig_patches_id.png&quot;, dpi = 300) knitr::include_graphics(&quot;figs/fig_patches_id.png&quot;) 8.6 Module metrics and composition ~ waterlevel How does the number of patches and number of ids and number of fixes change with waterlevel? # add patch data to get waterlevel data &lt;- read_csv(&quot;data/data_2018_patch_summary.csv&quot;) modules &lt;- inner_join(modules, data) # add rounded waterlevel mods_waterlevel &lt;- modules %&gt;% select(spatial_scale, time_scale, time_chunk, module, id, waterlevel_start) %&gt;% mutate(wlc = plyr::round_any(waterlevel_start, 50)) # get summary data by scale, time chunk, and waterlevel mods_waterlevel &lt;- mods_waterlevel %&gt;% group_by(spatial_scale, time_scale, time_chunk, wlc) %&gt;% summarise(n_patches = length(module), n_clusters = length(unique(module)), n_uid = length(unique(id))) %&gt;% pivot_longer(cols = c(&quot;n_patches&quot;, &quot;n_uid&quot;, &quot;n_clusters&quot;)) %&gt;% split(.$name) # svae fig list_plot &lt;- map2(mods_waterlevel, letters[1:3], function(df, title){ ggplot(df, aes(value))+ geom_histogram(fill = &quot;steelblue&quot;)+ # geom_pointrange(size = 0.2, col = &quot;steelblue&quot;)+ theme_test(base_size = 8)+ facet_grid(~wlc, scales = &quot;free_y&quot;, as.table = F, switch = &quot;both&quot;, labeller = label_both)+ # xlim(0, 20)+ labs(x = glue::glue(&#39;{unique(df$name)}&#39;), y = NULL, title = glue::glue(&#39;({title})&#39;))}) fig_mods_waterlevel &lt;- patchwork::wrap_plots(list_plot, ncol = 1) ggsave(fig_mods_waterlevel, filename = &quot;figs/fig_mods_waterlevel.png&quot;, device = png(), dpi = 300, height = 5, width = 5) knitr::include_graphics(&quot;figs/fig_mods_waterlevel.png&quot;) 8.7 Visualise modules # select single time chunk a = ggplot()+ geom_point(data = mod_data %&gt;% filter(between(time_chunk, 700, 750)) %&gt;% distinct(x_mean, y_mean, .keep_all = T), aes(x_mean, y_mean, # fill = duration, # col = factor(spatial_scale), size = area), shape = 1, col = &quot;grey20&quot;, alpha = 0.5, stroke = 0.15)+ geom_segment(data = mod_data %&gt;% filter(between(time_chunk, 700, 750)), aes(x_mean, y_mean, xend = xc, yend = yc, col = factor(spatial_scale)), size = 0.2, alpha = 0.8, show.legend = FALSE)+ # scale_fill_distiller(palette = &quot;Reds&quot;)+ scale_colour_brewer(palette = &quot;Set1&quot;)+ coord_sf(crs = 32631)+ # facet_wrap(~time_chunk)+ theme(axis.text = element_blank()) figplotly &lt;- ggplotly(a) saveWidget(figplotly, file = &quot;fig_modules.html&quot;) "],
["individual-traits-and-presence-in-modules.html", "Section 9 Individual traits and presence in modules 9.1 Prepare libraries 9.2 Read in module data 9.3 Count unique modules per individual 9.4 Link modules to individual data 9.5 Add patch data 9.6 Link independent patches to traits", " Section 9 Individual traits and presence in modules 9.1 Prepare libraries library(tidyverse) # for plots library(ggplot2) # ci function ci &lt;- function(x){qnorm(0.975)*sd(x, na.rm = TRUE)/sqrt(length(x))} 9.2 Read in module data # read in data modules &lt;- read_csv(&quot;data/data_2018_patch_modules_small_scale.csv&quot;) data &lt;- read_csv(&quot;data/data_2018_good_patches.csv&quot;) Choose three spatial scales and one temporal scale. modules &lt;- filter(modules, time_scale == 1, spatial_scale &lt;= 250) 9.3 Count unique modules per individual # get modules per id over the tidal cycle modules_per_id &lt;- inner_join(modules, data) # get mean waterlevel_start per module wlc_per_time_chunk &lt;- modules_per_id %&gt;% group_by(spatial_scale, module, time_chunk) %&gt;% summarise(wlc = plyr::round_any(mean(waterlevel_start), 50)) # count modules per id modules_per_id &lt;- modules %&gt;% group_by(id, spatial_scale, time_chunk) %&gt;% summarise(n_modules = length(unique(module))) # add wlc to the timechunk modules_per_id &lt;- left_join(modules_per_id, wlc_per_time_chunk) # get mean and ci modules per id modules_per_id &lt;- modules_per_id %&gt;% group_by(id, spatial_scale, wlc) %&gt;% summarise_at(vars(n_modules), list(mean=mean, ci=ci)) 9.4 Link modules to individual data # load physio data tag_data &lt;- read_csv(&quot;data/data_2018_behav.csv&quot;) modules_per_id &lt;- left_join(modules_per_id, tag_data, by = &quot;id&quot;) modules_per_id &lt;- filter(modules_per_id, !is.na(exploreScore)) # exclude extreme birds quantile(tag_data$exploreScore, probs = c(0.05, 0.95), na.rm = T) modules_per_id &lt;- filter(modules_per_id, between(exploreScore, -0.18, 0.74)) Select some possible predictors modules_per_id &lt;- modules_per_id %&gt;% select(spatial_scale, wlc, mean, ci, WING, MASS, gizzard_mass, exploreScore) %&gt;% pivot_longer(cols = c(&quot;WING&quot;, &quot;MASS&quot;, &quot;gizzard_mass&quot;, &quot;exploreScore&quot;)) modules_per_id &lt;- group_by(modules_per_id, name) %&gt;% mutate(value = scales::rescale(value)) # plot figure fig_mods_per_trait &lt;- ggplot(modules_per_id, aes(value, mean, ymin = mean-ci, ymax = mean+ci, col = factor(spatial_scale)))+ facet_grid(wlc~name, #xscales = &quot;free_x&quot;, switch = &quot;both&quot;, as.table = F, labeller = label_both)+ geom_pointrange(size = 0.05, position = position_dodge(width = 0.2))+ geom_smooth(method = &quot;glm&quot;, size = 0.1)+ scale_colour_brewer(palette = &quot;Set1&quot;)+ coord_equal(ratio = 0.5, xlim = c(0,1))+ labs(x = &quot;trait value (scaled 0 - 1)&quot;, y = &quot;number of modules per id&quot;, colour = &quot;spatial scale&quot;)+ theme_test(base_size = 8)+ theme(legend.position = &quot;top&quot;, panel.grid.major = element_line(size = 0.2, colour = &quot;grey&quot;)) # save figure ggsave(fig_mods_per_trait, filename = &quot;figs/fig_mods_per_trait.png&quot;) knitr::include_graphics(&quot;figs/fig_mods_per_trait.png&quot;) 9.5 Add patch data # split data by spatial scale mod_data &lt;- split(modules, modules$spatial_scale) # add patch data at each scale mod_data &lt;- map(mod_data, function(df){ full_join(df, data) }) # within each scale count patches with no module mod_data_indep &lt;- map(mod_data, function(df){ # identify waterlevel in 50cm increments df &lt;- mutate(df, wlc = plyr::round_any(waterlevel_start, 50)) df &lt;- group_by(df, id, wlc) %&gt;% summarise(indep_patches = sum(is.na(spatial_scale)), p_indep_patches = indep_patches/length(patch)) %&gt;% group_by(id, wlc) %&gt;% summarise_at(vars(p_indep_patches), list(mean_indep=mean, ci=ci)) }) # add spatial scale mod_data_indep &lt;- imap(mod_data_indep, ~ mutate(.x, spatial_scale = .y)) %&gt;% bind_rows() 9.6 Link independent patches to traits mod_data_indep &lt;- left_join(mod_data_indep, tag_data, by = &quot;id&quot;) mod_data_indep &lt;- filter(mod_data_indep, !is.na(exploreScore)) # exclude extreme birds quantile(tag_data$exploreScore, probs = c(0.05, 0.95), na.rm = T) mod_data_indep &lt;- filter(mod_data_indep, between(exploreScore, -0.18, 0.74)) # melt data mod_data_indep &lt;- select(mod_data_indep, spatial_scale, wlc, mean_indep, ci, WING, MASS, gizzard_mass, exploreScore) %&gt;% pivot_longer(cols = c(&quot;WING&quot;, &quot;MASS&quot;, &quot;gizzard_mass&quot;, &quot;exploreScore&quot;)) # make factor of spatial scale mod_data_indep$spatial_scale &lt;- as_factor(mod_data_indep$spatial_scale) mod_data_indep &lt;- group_by(mod_data_indep, name) %&gt;% mutate(value = scales::rescale(value)) fig_indep_patches &lt;- ggplot(mod_data_indep, aes(value, mean_indep, ymin = mean_indep-ci, ymax = mean_indep+ci, col = factor(spatial_scale)))+ geom_abline(slope = 1, col = &quot;black&quot;, alpha = 0.5, size = 0.1)+ # geom_hline(yintercept = 0.5, col = &quot;black&quot;, # alpha = 0.5, size = 0.2)+ facet_grid(wlc~name,# scales = &quot;free_x&quot;, as.table = F, switch = &quot;both&quot;, labeller = label_both)+ geom_pointrange(size = 0.05, position = position_dodge(width = 0.2))+ geom_smooth(method = &quot;glm&quot;, size = 0.1)+ scale_y_continuous(labels = scales::percent)+ scale_colour_brewer(palette = &quot;Set1&quot;)+ coord_equal(xlim=c(0,1))+ labs(x = &quot;trait value (scaled 0 - 1)&quot;, y = &quot;% patches not in modules&quot;, colour = &quot;spatial scale&quot;)+ theme_test(base_size = 8)+ theme(legend.position = &quot;top&quot;, panel.grid.major = element_line(size = 0.2, colour = &quot;grey&quot;)) ggsave(fig_indep_patches, filename = &quot;figs/fig_indep_patches.png&quot;) knitr::include_graphics(&quot;figs/fig_indep_patches.png&quot;) "],
["patch-metrics-in-relation-to-module-size.html", "Section 10 Patch metrics in relation to module size 10.1 Prepare libraries 10.2 Read in module data 10.3 Add patch data 10.4 Patch size ~ number of patches in a module 10.5 Get patches not in modules", " Section 10 Patch metrics in relation to module size 10.1 Prepare libraries library(tidyverse) # for plots library(ggplot2) library(plotly) library(htmlwidgets) # ci function ci &lt;- function(x){qnorm(0.975)*sd(x, na.rm = TRUE)/sqrt(length(x))} 10.2 Read in module data # read in data modules &lt;- read_csv(&quot;data/data_2018_patch_modules_small_scale.csv&quot;) data &lt;- read_csv(&quot;data/data_2018_good_patches.csv&quot;) Choose the smallest temporal scale. modules &lt;- filter(modules, time_scale == 1) 10.3 Add patch data # get mean and ci for patches mod_data &lt;- modules %&gt;% inner_join(data) # count unique ids in each mod-time-chunk and filter on 2 mod_data &lt;- mod_data %&gt;% group_by(spatial_scale, module, time_chunk) %&gt;% mutate(xc = mean(x_mean, na.rm = T), yc = mean(y_mean, na.rm = T), wlc = mean(waterlevel_start), wlc = plyr::round_any(wlc, 50), n_uid = length(unique(id)), n_patches = length(unique(x_mean))) Filter for minimum 2 indiviudals per module, and minimum of 3 patches. mod_data &lt;- filter(mod_data, n_uid &gt; 1, n_patches &gt; 3, !is.na(duration), spatial_scale &lt;= 100) 10.4 Patch size ~ number of patches in a module # count patches in module, and get mean patch area, duration, circ # summarise data mod_data_summary &lt;- mod_data %&gt;% mutate(n_patches = plyr::round_any(n_patches, 1)) %&gt;% group_by(spatial_scale, wlc, n_patches) %&gt;% # summarise_at(vars(area, duration, circularity, distBwPatch), # list(~mean(., na.rm = T))) %&gt;% pivot_longer(cols = c(&quot;area&quot;, &quot;duration&quot;, &quot;circularity&quot;, &quot;distBwPatch&quot;, &quot;distInPatch&quot;, &quot;dispInPatch&quot;)) %&gt;% drop_na() %&gt;% group_by(spatial_scale, wlc, n_patches, name) %&gt;% summarise_at(vars(value), list(~mean(., na.rm = T), ~ci(.))) # remove data above the 95% percentile quantile(mod_data_summary$n_patches, probs = c(0.05, 0.90)) # get by n patch mod_data_summary &lt;- mod_data_summary %&gt;% filter(n_patches &lt;= 20) 10.5 Get patches not in modules # split modules by spatial scale tmp_modules &lt;- filter(modules, spatial_scale &lt;= 100) tmp_modules &lt;- split(tmp_modules, tmp_modules$spatial_scale) indep_patches &lt;- map(tmp_modules, function(df){ tmp_df &lt;- anti_join(data, df) tmp_df &lt;- mutate(tmp_df, spatial_scale = unique(df$spatial_scale), n_patches = 1, wlc = plyr::round_any(waterlevel_start, 50)) # summarise metrics tmp_df &lt;- pivot_longer(tmp_df, cols = c(&quot;area&quot;, &quot;duration&quot;, &quot;circularity&quot;, &quot;distBwPatch&quot;, &quot;distInPatch&quot;, &quot;dispInPatch&quot;)) %&gt;% drop_na() %&gt;% group_by(spatial_scale, wlc, n_patches, name) %&gt;% summarise_at(vars(value), list(~mean(., na.rm = T), ~ci(.))) return(tmp_df) }) indep_patches &lt;- bind_rows(indep_patches) # add indep patches mod_data_summary &lt;- bind_rows(mod_data_summary, indep_patches) # make list mod_data_summary &lt;- split(mod_data_summary, mod_data_summary$name) # do plot over list list_plot &lt;- map2(mod_data_summary, letters[1:6], function(df, name){ ggplot(df, aes(n_patches, mean))+ geom_point(aes(fill = factor(spatial_scale)), size = ifelse(df$n_patches &gt; 1, 1, 2), shape = ifelse(df$n_patches &gt; 1, 2, 21), show.legend = F)+ geom_errorbar(aes(ymin = mean - ci, ymax = mean+ci, col = factor(spatial_scale)), width = 1, size = 0.2, show.legend = F)+ geom_smooth(method = &quot;glm&quot;, se = F, col = &quot;black&quot;, size = 0.2)+ scale_colour_brewer(palette = &quot;Set1&quot;)+ scale_x_continuous(breaks = c(1, seq(3,20,3)))+ theme_test(base_size = 8)+ theme(legend.position = &quot;none&quot;)+ facet_grid(spatial_scale~wlc, labeller = label_both, # scales = &quot;free_y&quot;, as.table = FALSE, switch = &quot;both&quot;)+ labs(x = &quot;# patches&quot;, y = glue::glue(&#39;{unique(df$name)}&#39;), title = glue::glue(&#39;({name})&#39;)) }) fig_metrics_social &lt;- patchwork::wrap_plots(list_plot, ncol = 2) ggsave(fig_metrics_social, filename = &quot;figs/fig_metrics_social.png&quot;, dpi = 300, height = 12, width = 12) knitr::include_graphics(&quot;figs/fig_metrics_social.png&quot;) "],
["patch-metrics-inside-and-outside-modules.html", "Section 11 Patch metrics inside and outside modules", " Section 11 Patch metrics inside and outside modules "],
["bathymetry-of-the-griend-mudflats.html", "Section 12 Bathymetry of the Griend mudflats 12.1 Get data and plot basic trend 12.2 Plot as 3D maps", " Section 12 Bathymetry of the Griend mudflats 12.1 Get data and plot basic trend # load libs to read bathymetry library(raster) library(rayshader) library(sf) # data libs library(data.table) library(purrr) library(stringr) # plot libs library(ggplot2) library(ggthemes) # load bathymetry and subset data &lt;- raster(&quot;data/bathymetry_waddenSea_2015.tif&quot;) griend &lt;- st_read(&quot;griend_polygon/griend_polygon.shp&quot;) %&gt;% st_buffer(10.5e3) # assign utm 31 crs and subset crs(data) &lt;- unlist(st_crs(griend)[2]) data &lt;- crop(data, as(griend, &quot;Spatial&quot;)) data &lt;- aggregate(data, fact = 2) # remove 20m outliers #data[data &gt; 500] &lt;- NA data_m &lt;- raster_to_matrix(data) # get quantiles of the matrix data_q &lt;- quantile(data_m, na.rm = T, probs = 1:1000/1000) # read in waterlevel data waterlevel &lt;- fread(&quot;data/data2018/waterlevelWestTerschelling.csv&quot;, sep = &quot;;&quot;) # select useful columns and rename waterlevel &lt;- waterlevel[,.(WAARNEMINGDATUM, WAARNEMINGTIJD, NUMERIEKEWAARDE)] setnames(waterlevel, c(&quot;date&quot;, &quot;time&quot;, &quot;level&quot;)) # make a single POSIXct column of datetime waterlevel[,dateTime := as.POSIXct(paste(date, time, sep = &quot; &quot;), format = &quot;%d-%m-%Y %H:%M:%S&quot;, tz = &quot;CET&quot;)] waterlevel &lt;- setDT(distinct(setDF(waterlevel), dateTime, .keep_all = TRUE)) # plot waterlevel quantiles with data from west terschelling waterlimits &lt;- range(waterlevel$level) fig_waterlevel_area &lt;- ggplot()+ geom_line(aes(x = 1:1000/1000, y = data_q))+ geom_hline(yintercept = waterlimits, col = &quot;blue&quot;, lty = 2)+ geom_hline(yintercept = 0, lty = 2, col = &quot;red&quot;)+ scale_x_continuous(labels = scales::percent)+ theme_bw()+ coord_flip(xlim = c(0.25, 1.01), ylim = c(-250, 250))+ labs(x = &quot;% area covered&quot;, y = &quot;waterlevel (cm over NAP)&quot;) ggsave(fig_waterlevel_area, filename = &quot;figs/fig_waterlevel_area.png&quot;, dpi = 300, height = 4, width = 6); dev.off() 12.2 Plot as 3D maps # make sequence waterdepth &lt;- seq(waterlimits[1], waterlimits[2], length.out = 30) waterdepth &lt;- c(waterdepth, rev(waterdepth)) # make visualisation for(i in 1:length(waterdepth)) { data_m %&gt;% sphere_shade(texture = &quot;imhof1&quot;, zscale = 50) %&gt;% # add_water(detect_water(data_m, zscale = 100, # max_height = waterdepth[i],cutoff = 0.1), # color = &quot;desert&quot;) %&gt;% add_shadow(hillshade = data_m) %&gt;% plot_3d(data_m, zscale = 75, wateralpha = 0.6, solid =F, shadow=F, water = TRUE, waterdepth = waterdepth[i], watercolor = &quot;dodgerblue1&quot;, phi = 90, theta = 0, zoom = 0.75, windowsize = c(1000, 800), background = &quot;black&quot;, calculate_normals = F) render_label(data_m, x = 500, y = 500, z = 500, text = glue::glue(&#39;waterdepth = {round(waterdepth[i])} cm&#39;), freetype = F, textcolor = &quot;black&quot;) rgl::snapshot3d(paste0(&quot;figs/tide_rise/fig&quot;,str_pad(i, 2, pad = &quot;0&quot;),&quot;.png&quot;)) rgl::rgl.close() } library(magick) list.files(path = &quot;figs/tide_rise/&quot;, pattern = &quot;*.png&quot;, full.names = T) %&gt;% map(image_read) %&gt;% # reads each path file image_join() %&gt;% # joins image image_animate(fps=2) %&gt;% # animates, can opt for number of loops image_write(&quot;figs/fig_tide_rise_anim.gif&quot;) # write to current dir if (knitr:::is_latex_output()) { } else { knitr::include_graphics(&quot;figs/fig_tide_rise_anim.gif&quot;) } (#fig:include_gif)Waterlevel at West Terschelling and effect on coverage of mudflats around Griend. "]
]
